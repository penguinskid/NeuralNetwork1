# -*- coding: utf-8 -*-
"""
Created on Mon Jan 25 17:29:39 2016

@author: Simon
"""

import numpy as np
import pandas as pd
from openpyxl import load_workbook

# I set the seed so that my work could be reproduced later if need be.
np.random.seed(420)

#These are the X and Y matrices, predictors and outputs respectively, in simply numpy arrays.
#There are no column headers, but in the future I might think about using a pandas dataframe so I
#can keep track of the column names/features/insert data science column header name.  For now, I just know
#that the predictors are dollars and manpower, and the output are all the function's AF COLS data.
X= np.genfromtxt('MPFUNDEDMANPOWERANDPRICING_ONLYDATAVALUES.csv', delimiter = ',')
Y = np.genfromtxt('MPMETRICOUTPUTS_ONLYDATAVALUES.csv', delimiter = ',')


RP = pd.read_excel('NN_MP_RunParameters.xlsx', sheetname= "NN_MP_RunParameters", header=0)

# All of the code below this comment is here so that I can add sheets of output data to a file called
#NN_IM_RunParameters.  This file contains the parameters of the neural network (not including weights)
# and also the output so I can compare performance and reproduce results, all located in one single file
book = load_workbook('NN_MP_RunParameters.xlsx')
OutputFile = pd.ExcelWriter('NN_MP_RunParameters.xlsx', engine = 'openpyxl')
OutputFile.book = book
OutputFile.sheets = dict((ws.title, ws) for ws in book.worksheets)

# I concatenated the data so that I could shuffle it (randomize it) in the next line, to reduce the bias
# on selecting the training and testing data
A = np.concatenate((X,Y), axis = 1)

#The code below randomly shuffles the data
np.random.shuffle(A)

#Here, I am counting the number of rows using A.shape[0], and then multiplying it by
#.8; I am aiming for the 80:20 split of training:testing data, and turning it into an integer
#so I can partition the data easily using a whole number that is slightly less than the actual A.shape[0]*.8
rowcount = int(A.shape[0]*.8)

#Here I break the data by the inputs and the outputs to fit the methods in the net
X, Y = np.split(A, [X.shape[1]], axis = 1)

trainX = X[0:rowcount]
trainY = Y[0:rowcount]

testX = X[rowcount:]
testY = Y[rowcount:]

trX = trainX/np.amax(trainX, axis=0)
trY = trainY/[100, 100, 100, 100,100,100,100]

teX = testX/np.amax(trainX, axis=0)
teY = testY/[100, 100, 100, 100,100,100,100]

class NN_AFCOLS_MP(object):
    # The activation variable is reserved to type in the activation function that you want to use
    # The two activation functions I have right now are the sigmoid function and the
    # hyperbolic tangent function.  More might be added if I find some that publications that demonstrate their viability.
    def __init__(self, Lambda = 0, activation = '', tol = 1e-5, alpha = 1, iterations=10000):
        #define HyperParameters
        self.inputLayerSize = trX.shape[1]
        self.hiddenLayer1Size = 12
        self.hiddenLayer2Size = 10
        self.outputLayerSize = trY.shape[1]
        
        self.activation = activation
        if activation == '':
            self.activation = 'sigmoid'
        self.Lambda = Lambda
        
        self.tol = tol
        
        self.alpha = alpha
        
        self.iterations = iterations
		
        #Weights (Parameters)
        np.random.seed(420)
        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayer1Size)
        self.W2 = np.random.randn(self.hiddenLayer1Size, self.hiddenLayer2Size)
        self.W3 = np.random.randn(self.hiddenLayer2Size, self.outputLayerSize)

    def forward(self, X):
        #Propagate inputs through network
        self.z2 = np.dot(X, self.W1)
        self.a2 = self.activationFunction(self.z2)
        self.z3 = np.dot(self.a2, self.W2)
        self.a3 = self.activationFunction(self.z3)
        self.z4 = np.dot(self.a3, self.W3)
        yHat = self.activationFunction(self.z4)
        return yHat

    def activationFunction(self, z):
        
        #Apply the activation function to scalar, vector, or matrix
        if self.activation == 'sigmoid':
            return 1/(1+np.exp(-z))
        elif self.activation == 'HT':
            return (np.exp(z)-np.exp(-z))/(np.exp(z) + np.exp(-z))

    def activationPrime(self, z):
        #Derivative of the activation Functions
        if self.activation == 'sigmoid':
            return np.exp(-z)/((1+np.exp(-z))**2)
        if self.activation == 'HT':
            return 4/(np.exp(z) + np.exp(-z))**2

    def costFunction(self, X, y):
        
        #Compute cost for given X,y, use weights already stored in class.
        self.yHat = self.forward(X)
        J = 0.5*sum((y-self.yHat)**2)/X.shape[0] + (self.Lambda/2)*(sum(self.W1.ravel()**2)+sum(self.W2.ravel()**2)+sum(self.W3.ravel()**2))
        return J
        
    def costFunctionPrime(self, X, y):
        #Compute derivative with respect to the weights
        self.yHat = self.forward(X)
        
        delta4 = np.multiply(-(y-self.yHat), self.activationPrime(self.z4))
        dJdW3 = np.dot(self.a3.T, delta4)/X.shape[0] + self.Lambda*self.W3
        
        delta3 = np.dot(delta4, self.W3.T)*self.activationPrime(self.z3)
        dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + self.Lambda*self.W2
        
        delta2 = np.dot(delta3, self.W2.T)*self.activationPrime(self.z2)
        dJdW1 = np.dot(X.T, delta2)/X.shape[0] + self.Lambda*self.W1

        
        return dJdW1, dJdW2, dJdW3
        
    def getParams(self):
        
        #Get W1 and W1 Rolled into vector:
        params = np.concatenate((self.W1.ravel(), self.W2.ravel(), self.W3.ravel()))
        return params

    def computeGradients(self, X, y):
        
        dJdW1, dJdW2, dJdW3 = self.costFunctionPrime(X, y)
        return np.concatenate((dJdW1.ravel(), dJdW2.ravel(), dJdW3.ravel()))
    
    def gradientDescent(self, X, y):
        
        i = 1
        while i<self.iterations:
            dJdW1, dJdW2, dJdW3 = self.costFunctionPrime(X,y)
            self.W1 = self.W1-self.alpha*dJdW1
            self.W2 = self.W2-self.alpha*dJdW2
            self.W3 = self.W3-self.alpha*dJdW3
            if max(np.concatenate((dJdW1.ravel(), dJdW2.ravel(), dJdW3.ravel())))<self.tol:
                self.iterations = i             
                break
            i+=1
    
NN_MP = NN_AFCOLS_MP(Lambda = .0001, activation = "sigmoid", alpha = .1)
NN_MP.gradientDescent(trX, trY)
RP_new = [(len(RP.index)+1), NN_MP.activation, NN_MP.tol, NN_MP.alpha, NN_MP.Lambda, NN_MP.iterations]
if len(RP.index)==0:
    RP.loc[len(RP.index)]= RP_new
    a = NN_MP.forward(teX).ravel()
    b = teY.ravel()
    c = np.vstack((b,a))
    pd.DataFrame(RP).to_excel(OutputFile, sheet_name = 'NN_MP_RunParameters', index = False)
    pd.DataFrame(c).to_excel(OutputFile, sheet_name = 'NN_MP_Predictions'+str(len(RP.index)))
    OutputFile.save()
    OutputFile.close()
    print "Your parameters and output were succesfully saved to NN_IM_RunParameters in ID " + str(len(RP.index)) + "."
else:
    for i in range(0,len(RP.index)):
        if list(RP.loc[i][1:])==RP_new[1:]:
            print "These parameters have already been tested.  Check ID " + str(i+1) + " for an experiment identical to yours."
            break
        if i == (len(RP.index)-1):     
            RP.loc[len(RP.index)]= RP_new
            a = NN_MP.forward(teX).ravel()
            b = teY.ravel()
            c = np.vstack((b,a))
            pd.DataFrame(RP).to_excel(OutputFile, sheet_name = 'NN_MP_RunParameters', index = False)
            pd.DataFrame(c).to_excel(OutputFile, sheet_name = 'NN_MP_Predictions'+str(len(RP.index)))
            OutputFile.save()
            OutputFile.close()
            print "Your parameters and output were succesfully saved to NN_IM_RunParameters in ID " + str(len(RP.index)) + "."
